{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows another method for webscraping that tries to avoid getting blocked by the website.\n",
    "\n",
    "Selenium (our previous method) is a great tool for webscraping, but it's not the best option when you need to scrape a lot of pages from a website. The reason is that Selenium uses a real browser to make the requests, and this can be easily detected by the website, which can block your IP address.\n",
    "\n",
    "We're using the `httpx` library to make the requests and the `BeautifulSoup` library to parse the html content.\n",
    "\n",
    "We'll alse set a 'User-Agent' header to make the request look like it's coming from a real browser.\n",
    "\n",
    "This method is not perfect (how perfect a method is, is up to you), but it's a good alternative and very light to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use httpx to get the page\n",
    "import httpx\n",
    "\n",
    "url = 'https://www.tripadvisor.com/Hotel_Review-g187849-d2340336-Reviews-Armani_Hotel-Milan_Lombardy.html'\n",
    "response = httpx.get(url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})\n",
    "response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_text = response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(page_text, 'html.parser')\n",
    "\n",
    "for review in soup.find_all('div', attrs={'data-reviewid': True}):\n",
    "    review_id = review.get('data-reviewid')\n",
    "    review_rating = review.find('div', attrs={'data-test-target': \"review-rating\"}).find('title').text\n",
    "    review_rating = float(review_rating.split(' ')[0])\n",
    "    review_title = review.find('div', attrs={'data-test-target': \"review-title\"}).text\n",
    "    review_text = review.find('span', attrs={'data-automation': f\"reviewText_{review_id}\"}).text\n",
    "    print(review_id, review_rating, review_title.strip(), review_text.strip())\n",
    "\n",
    "    #get next page using data-smoke-attr=\"pagination-next-arrow\"\n",
    "next_page = soup.find('a', attrs={'data-smoke-attr': 'pagination-next-arrow'})\n",
    "print(next_page.get('href'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = 'https://www.tripadvisor.com'\n",
    "httpx.get(BASE_URL+next_page.get('href'), headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'}).text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 /Hotel_Review-g187849-d2340336-Reviews-or10-Armani_Hotel-Milan_Lombardy.html\n",
      "ended\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "BASE_URL = 'https://www.tripadvisor.com'\n",
    "\n",
    "def get_page(url):\n",
    "    response = httpx.get(BASE_URL+url, headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64)'})\n",
    "    page_text = response.text\n",
    "    soup = BeautifulSoup(page_text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "next_page = '/Hotel_Review-g187849-d2340336-Reviews-or10-Armani_Hotel-Milan_Lombardy.html'\n",
    "\n",
    "with open('data/ArmaniHotelReviews_202409.csv', 'w', encoding=\"utf-8\") as csv_file:\n",
    "    csvwriter = csv.writer(csv_file, lineterminator='\\n' )\n",
    "    csvwriter.writerow(['review_id', 'date_of_stay', 'review_rating', 'review_title', 'review_text'])\n",
    "    \n",
    "    page_num = 0\n",
    "    date_of_stay_class = '' # this is a class name is generated with random characters on the site, we will fetch it once\n",
    "    while len(next_page) > 0:# and page_num <5:\n",
    "        page_num += 1\n",
    "        print(page_num, next_page)\n",
    "        soup = get_page(next_page)\n",
    "\n",
    "        # check if the page is blocked by finding data-test-target=\"reviews-tab\". if found then page is not blocked\n",
    "        if not soup.find('div', attrs={'data-test-target': 'reviews-tab'}):\n",
    "            print('Page has been blocked')\n",
    "            break\n",
    "\n",
    "        for review in soup.find_all('div', attrs={'data-reviewid': True}):\n",
    "            review_id = review.get('data-reviewid')\n",
    "            review_rating = review.find('div', attrs={'data-test-target': \"review-rating\"}).find('title').text\n",
    "            review_rating = float(review_rating.split(' ')[0])\n",
    "            review_title = review.find('div', attrs={'data-test-target': \"review-title\"}).text\n",
    "            review_text = review.find('span', attrs={'data-automation': f\"reviewText_{review_id}\"}).text\n",
    "\n",
    "            if date_of_stay_class == '':\n",
    "                # date of stay is not easy to access. we need to get it by position within the first review\n",
    "                # first we get the 4th child div\n",
    "                # then the 2nd child of the current child div\n",
    "                # then the 1st child of the current child div - this should be a span\n",
    "                date_of_stay_class = review.contents[3].contents[1].contents[0].get('class')\n",
    "            \n",
    "            # now we can get the date of stay from the span\n",
    "            date_of_stay = review.contents[3].find('span', attrs={'class': date_of_stay_class}).text\n",
    "            #then we split the text to get just the date (example: \"Date of stay: August 2024\" -> \"August 2024\")\n",
    "            date_of_stay = date_of_stay.split(': ')[1]\n",
    "            \n",
    "            csvwriter.writerow([review_id, date_of_stay, review_rating, review_title.strip(), review_text.strip()])\n",
    "\n",
    "        #get next page using data-smoke-attr=\"pagination-next-arrow\"\n",
    "        next_page_nav = soup.find('a', attrs={'data-smoke-attr': 'pagination-next-arrow'})\n",
    "        if next_page_nav is not None:\n",
    "            next_page = next_page_nav.get('href', default='')\n",
    "        else:\n",
    "            next_page = ''\n",
    "\n",
    "print(\"ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
